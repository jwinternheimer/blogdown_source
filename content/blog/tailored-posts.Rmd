---
date: 2018-02-02T12:15:02-05:00
type: "post"
tags: []
title: "Measuring the Effect of Tailored Posts"
---

We've recently updated our composer in Buffer's web app to make it easier to customize messages for each social network. As of February 2, the new composer experience, called Tailored Posts, has been rolled out to approximately 50% of Buffer users.

![](http://hi.buffer.com/1S0V0W2H3b1a/Screen%20Shot%202018-02-02%20at%2012.18.18%20PM.png)

In this post we'll try to measure the effect that this new composer experience has had on scheduling behavior. More specifically, we'll analyze the effect on posting frequency, the number of social accounts shared to in a single composing sesion, and the similarity of posts that are scheduled.

```{r include = FALSE, message = FALSE, warning = FALSE}
# load libraries 
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(hrbrthemes)
library(data.table)
library(buffer)
```

### Data collection
We need to gather updates from composer sessions in which at least one Twitter profile was selected.

```{r include = FALSE}
# get redshift connection
con <- redshift_connect()
```

```{sql connection = con, output.var = posts}
with lifetime_updates as (
  select
    up.user_id
    , count(distinct up.id) as lifetime_posts
    , count(distinct up.profile_id) as profiles
  from dbt.updates as up
  left join dbt.users as u
    on up.user_id = u.user_id
  where up.was_sent_with_buffer
  and date(u.created_at) >= (current_date - 31)
  and up.has_failed = false
  group by up.user_id
), 

user_facts as (
  select
    u.user_id
    , u.created_at as user_created_at
    , u.features
    , lu.lifetime_posts
    , lu.profiles
  from dbt.users as u
  left join lifetime_updates as lu
    on u.user_id = lu.user_id
  where u.created_at >= (current_date - 31)
)

select
  up.id
  , u.user_id
  , u.user_created_at
  , u.features
  , u.profiles
  , u.lifetime_posts
  , up.profile_id
  , up.client_id
  , up.created_at
  , up.sent_at
  , up.status
  , up.profile_service
  , up.via
  , up.client_type
  , up.text
  , up.has_text
  , up.has_hashtag
  , up.is_retweet
  , up.has_photo
  , up.has_multiple_photos
  , up.has_video
from dbt.updates as up
inner join user_facts as u
  on up.user_id = u.user_id
```

Great, we have over two million updates to work with. Now we need to gather the `composer_event_id` in order to tell which updates were sent in the same composer window.

```{sql connection = con, output.var = composer_events}
with step1 as (
  select
    a.user_id
    , a.date
    , a.id as event_id
    , case
        when a.full_scope like 'extension updates shared composer%' then 'simple_extension'
        when a.full_scope like 'extension composer multiple-composers updates shared%' then 'multiple_extension'
      end as composer_type
    , json_extract_path_text(a.extra_data, 'updates_ids') as updates
  from actions_taken a
  where a.full_scope like 'extension updates shared composer%'
  or a.full_scope like 'extension composer multiple-composers updates shared%'
  and a.date >= (current_date - 31)
  and json_extract_path_text(a.extra_data, 'updates_ids') != ''
  group by 1,2,3,4,5
), 

step2 as (
  select
    *
    , json_extract_array_element_text(s.updates, seq.i) as update_id
  from step1 s
  inner join seq_0_to_300 seq on seq.i < json_array_length(s.updates)
)
select * from step2
```


Let's filter to only include users that have scheduled at least 5 posts with Bufer. We also want to make sure that the posts were scheduled from the dashboard, and not from the extensions or third party apps like IFTTT.

```{r}
# filter out users that have scheduled less than 5 posts
posts <- posts %>% 
  filter(lifetime_posts >= 5 & via == "api")
```

Now we need to determine if the user has Tailored Posts enabled on their account. 

```{r}
# determine if tailored posts are enabled
posts <- posts %>% 
  mutate(tp_enabled = features %like% "mc-dashboard") 

# check how many have TP enabled
count(posts, tp_enabled)
```

Approximately 20% of the posts were sent by users with Tailored Posts enabled. Next we need to determine if two posts were scheduled in the same composer session. This is slightly tricky, as we don't have a composer event ID, and posts scheduled in the same composer session can have different `created_at` values. They are usually within 10 seconds of each other, however. We can use this to our advantage. 

The first thing we need to do is find the number of seconds that elapse between an update and the same user's next update. We'll use `dplyr`'s `lead` function to get the time of the user's next update, and create a new column `diff` that calculates the number of seconds that elapsed between posts.

```{r}
# calculate time of the previous post
prev_post <- posts %>% 
  arrange(user_id, created_at) %>% 
  group_by(user_id) %>% 
  mutate(last_update = lag(created_at, 1, order_by = user_id),
         diff = as.numeric(created_at - last_update)) %>% 
  select(user_id, id, created_at, last_update, diff) %>% 
  arrange(user_id, created_at)

# view some of the ddata
head(prev_post)
```

Now that we have the number of time that elapsed between updates, we'll want to group updates that were created within 10 seconds of each other. How can we do this? At the moment, I don't know. Let's move on for now and circle back if we are able to get the composer event IDs for the updates.

```{r}
prev_post <- prev_post %>% 
  mutate(same_composer = ifelse(diff <= 10, 1, 0), 
         post_count = ifelse(same_composer == 1, 
                        sequence(rle(as.character(same_composer))$lengths), 0))
```

### Posting frequency
Let's examine the number of times users post per week. We'll assume that the distribution of the number of profiles for users in each group is roughly the same. First let's create a `week` column from the `created_at` column using `lubridate`.

```{r}
# get the week
posts <- posts %>% 
  mutate(week = floor_date(created_at, unit = 'weeks'))
```

Now let's count the number of updates sent by each user each week.

```{r}
# get posts per week
posts_per_week <- posts %>% 
  group_by(user_id, user_created_at, tp_enabled, week) %>% 
  summarise(updates = n_distinct(id))
```

It's important to remember that weeks in which users sent no updates _are not_ included in this dataframe. We could include those using `tidyr`'s `complete` function, but we would run the risk of including users in weeks after they have already effective churned. I'm ok with this because we have only queried for users created in the past 30 days.

```{r}
# complete the dataframe
posts_per_week <- posts_per_week %>% 
  complete(week, user_id, fill = list(updates = 0))
```

Cool. Now we filter out weeks that came before users signed up and calculate the number of updates sent per week for each user.

```{r}
# calculate posts per user 
by_user <- posts_per_week %>% 
  filter(week >= floor_date(user_created_at, unit = 'weeks')) %>% 
  group_by(user_id, tp_enabled) %>% 
  summarise(weeks = n_distinct(week), updates = sum(updates)) %>% 
  mutate(updates_per_week = updates, weeks)
```

Now we can calculate a few summary statistics.

```{r}
# calculate summary statistics
by_user %>% 
  group_by(tp_enabled) %>% 
  summarise(avg_updates = mean(updates_per_week),
            med_updates = median(updates_per_week),
            sd = sd(updates_per_week))
```

We can see that the variance in the number of updates sent per week is much higher for folks with Tailored Posts enabled. Let's plot the distribution of updates per week for both groups.

```{r echo = FALSE, warning = FALSE, message = FALSE}
# plot distributions
ggplot(by_user, aes(x = updates_per_week, fill = tp_enabled)) +
  geom_density(aes(color = tp_enabled), alpha = 0.3) +
  scale_x_continuous(limits = c(0, 100)) +
  theme_ipsum() +
  scale_fill_discrete(guide = FALSE) +
  labs(x = "Updates Per Week", y = NULL, 
       color = "", title = "Distribution of Updates Per Week")
```

This plot seems to suggest that a greater proportion of users _without_ tailored posts enabled have scheduled around 10 posts per week. 

